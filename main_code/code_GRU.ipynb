{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:28.157843Z",
     "start_time": "2021-05-17T05:27:26.973083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:30.667659Z",
     "start_time": "2021-05-17T05:27:28.749980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparate GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from tensorflow.keras import optimizers # 케라스의 옵티마이저를 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:30.759656Z",
     "start_time": "2021-05-17T05:27:30.745604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def make_dataset(data, label, window_size):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size+1):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size-1]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "def make_FCset(data, window_size):\n",
    "    feature_list = []\n",
    "    for i in range(len(data) - window_size+1):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "    return np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:31.039433Z",
     "start_time": "2021-05-17T05:27:31.025380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the input data\n",
    "data_ori= pd.read_csv('total_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:31.677357Z",
     "start_time": "2021-05-17T05:27:31.665749Z"
    }
   },
   "outputs": [],
   "source": [
    "data_ori.index=range(len(data_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:32.017062Z",
     "start_time": "2021-05-17T05:27:32.003171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'Time', 'Year', 'Week', 'Month', 'Rate', 'Mean temperature',\n",
       "       'Minimum temperature', 'Maximum temperature', 'Rainfall',\n",
       "       'Minimum huminity', 'Relative huminity', 'Day length',\n",
       "       'Duration of sunshine', 'Soil temperature at 3m',\n",
       "       'Soil temperature at 5m', 'Number of Daycare center',\n",
       "       'Population of Daycare center', 'outbreak', 'warning', 'peak',\n",
       "       'warning3', 'warning2', 'season', 'last_rate', 'th'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ori.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:33.597972Z",
     "start_time": "2021-05-17T05:27:33.580681Z"
    }
   },
   "outputs": [],
   "source": [
    "targets=['warning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'Time', 'Year', 'Week', 'Month', 'Rate', 'Mean temperature',\n",
       "       'Minimum temperature', 'Maximum temperature', 'Rainfall',\n",
       "       'Minimum huminity', 'Relative huminity', 'Day length',\n",
       "       'Duration of sunshine', 'Soil temperature at 3m',\n",
       "       'Soil temperature at 5m', 'Number of Daycare center',\n",
       "       'Population of Daycare center', 'outbreak', 'warning', 'peak',\n",
       "       'warning3', 'warning2', 'season', 'last_rate', 'th'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ori.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:33.970516Z",
     "start_time": "2021-05-17T05:27:33.963528Z"
    }
   },
   "outputs": [],
   "source": [
    "variables=['Minimum temperature',\n",
    "       'Rainfall', 'Relative humidity', 'Day length',\n",
    "       'Soil temperature at 3m',\n",
    "       'Soil temperature at 5m', 'last_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:35.395311Z",
     "start_time": "2021-05-17T05:27:35.380286Z"
    }
   },
   "outputs": [],
   "source": [
    "# model input\n",
    "u_data=data_ori\n",
    "scaler=MinMaxScaler()\n",
    "window_size=3\n",
    "layer_num=50\n",
    "TT_year=2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "data_train=data_ori[data_ori['Year']<TT_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "data_test=data_ori[len(data_train)-window_size+1:]\n",
    "data_test.index=range(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_train[variables+targets]\n",
    "test = data_test[variables+targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = variables\n",
    "label_cols = ['warning']\n",
    "\n",
    "train_feature = train[feature_cols]\n",
    "train_label = train[label_cols]\n",
    "\n",
    "test_feature = test[feature_cols]\n",
    "test_label = test[label_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess train set\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test set\n",
    "test_feature, test_label = make_dataset(test_feature, test_label, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare result save\n",
    "result_GRU=pd.DataFrame(data_train[['Year','Week','warning']])\n",
    "predict_GRU=pd.DataFrame(data_test[['Year','Week','warning']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_GRU=result_GRU[2:]\n",
    "result_GRU.index=range(len(result_GRU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_GRU=predict_GRU[2:]\n",
    "predict_GRU.index=range(len(predict_GRU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:50.929532Z",
     "start_time": "2021-05-17T05:27:41.241335Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sieunlee/miniforge3/envs/noro/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2022-02-18 23:17:11.264247: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-02-18 23:17:11.267018: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-02-18 23:17:11.538442: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 58ms/step - loss: 1.8200 - val_loss: 0.6526\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65263, saving model to tmp_checkpoint.h5\n",
      "Epoch 2/5000\n",
      " 4/12 [=========>....................] - ETA: 0s - loss: 0.6955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 23:17:12.508805: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 25ms/step - loss: 0.5298 - val_loss: 0.2682\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65263 to 0.26815, saving model to tmp_checkpoint.h5\n",
      "Epoch 3/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.2188 - val_loss: 0.2376\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26815 to 0.23763, saving model to tmp_checkpoint.h5\n",
      "Epoch 4/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.1712 - val_loss: 0.1865\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.23763 to 0.18649, saving model to tmp_checkpoint.h5\n",
      "Epoch 5/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.1293 - val_loss: 0.1527\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.18649 to 0.15268, saving model to tmp_checkpoint.h5\n",
      "Epoch 6/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.1289 - val_loss: 0.1460\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15268 to 0.14603, saving model to tmp_checkpoint.h5\n",
      "Epoch 7/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.1061 - val_loss: 0.1465\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.14603\n",
      "Epoch 8/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.1023 - val_loss: 0.1201\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14603 to 0.12015, saving model to tmp_checkpoint.h5\n",
      "Epoch 9/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.1046 - val_loss: 0.1149\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12015 to 0.11488, saving model to tmp_checkpoint.h5\n",
      "Epoch 10/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0899 - val_loss: 0.1121\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11488 to 0.11211, saving model to tmp_checkpoint.h5\n",
      "Epoch 11/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0918 - val_loss: 0.1170\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11211\n",
      "Epoch 12/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0862 - val_loss: 0.1161\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11211\n",
      "Epoch 13/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0913 - val_loss: 0.1615\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11211\n",
      "Epoch 14/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0992 - val_loss: 0.1098\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11211 to 0.10977, saving model to tmp_checkpoint.h5\n",
      "Epoch 15/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0909 - val_loss: 0.1325\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.10977\n",
      "Epoch 16/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0919 - val_loss: 0.1183\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10977\n",
      "Epoch 17/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.1056 - val_loss: 0.1073\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.10977 to 0.10731, saving model to tmp_checkpoint.h5\n",
      "Epoch 18/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.1011 - val_loss: 0.1684\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10731\n",
      "Epoch 19/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.1022 - val_loss: 0.1184\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10731\n",
      "Epoch 20/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0989 - val_loss: 0.1870\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10731\n",
      "Epoch 21/5000\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.1044 - val_loss: 0.1213\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10731\n",
      "Epoch 22/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.1043 - val_loss: 0.1173\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10731\n",
      "Epoch 23/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0886 - val_loss: 0.1453\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10731\n",
      "Epoch 24/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0850 - val_loss: 0.1037\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10731 to 0.10371, saving model to tmp_checkpoint.h5\n",
      "Epoch 25/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0851 - val_loss: 0.1134\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10371\n",
      "Epoch 26/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0859 - val_loss: 0.1016\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10371 to 0.10158, saving model to tmp_checkpoint.h5\n",
      "Epoch 27/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0958 - val_loss: 0.1003\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10158 to 0.10029, saving model to tmp_checkpoint.h5\n",
      "Epoch 28/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0897 - val_loss: 0.1028\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10029\n",
      "Epoch 29/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0746 - val_loss: 0.1150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10029\n",
      "Epoch 30/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0776 - val_loss: 0.1010\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10029\n",
      "Epoch 31/5000\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0767 - val_loss: 0.1065\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10029\n",
      "Epoch 32/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0761 - val_loss: 0.1302\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10029\n",
      "Epoch 33/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0784 - val_loss: 0.1031\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10029\n",
      "Epoch 34/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0756 - val_loss: 0.1475\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.10029\n",
      "Epoch 35/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0775 - val_loss: 0.1203\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10029\n",
      "Epoch 36/5000\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0742 - val_loss: 0.0954\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.10029 to 0.09535, saving model to tmp_checkpoint.h5\n",
      "Epoch 37/5000\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0765 - val_loss: 0.0969\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09535\n",
      "Epoch 38/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0746 - val_loss: 0.0979\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09535\n",
      "Epoch 39/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0740 - val_loss: 0.1538\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09535\n",
      "Epoch 40/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0803 - val_loss: 0.0995\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09535\n",
      "Epoch 41/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0853 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09535\n",
      "Epoch 42/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.1070 - val_loss: 0.1675\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09535\n",
      "Epoch 43/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0970 - val_loss: 0.1103\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09535\n",
      "Epoch 44/5000\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0817 - val_loss: 0.0992\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09535\n",
      "Epoch 45/5000\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0759 - val_loss: 0.1310\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09535\n",
      "Epoch 46/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0893 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09535 to 0.09438, saving model to tmp_checkpoint.h5\n",
      "Epoch 47/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0760 - val_loss: 0.1224\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09438\n",
      "Epoch 48/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0731 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09438 to 0.09232, saving model to tmp_checkpoint.h5\n",
      "Epoch 49/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0697 - val_loss: 0.0973\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09232\n",
      "Epoch 50/5000\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0732 - val_loss: 0.1417\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09232\n",
      "Epoch 51/5000\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.1076 - val_loss: 0.1457\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.09232\n",
      "Epoch 52/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0875 - val_loss: 0.1083\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09232\n",
      "Epoch 53/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0702 - val_loss: 0.1029\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09232\n",
      "Epoch 54/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0733 - val_loss: 0.1383\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.09232\n",
      "Epoch 55/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0838 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.09232 to 0.09157, saving model to tmp_checkpoint.h5\n",
      "Epoch 56/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0686 - val_loss: 0.1048\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.09157\n",
      "Epoch 57/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0687 - val_loss: 0.1007\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.09157\n",
      "Epoch 58/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0843 - val_loss: 0.1357\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.09157\n",
      "Epoch 59/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0748 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09157 to 0.08980, saving model to tmp_checkpoint.h5\n",
      "Epoch 60/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0704 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.08980 to 0.08715, saving model to tmp_checkpoint.h5\n",
      "Epoch 61/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0806 - val_loss: 0.1186\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.08715\n",
      "Epoch 62/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0685 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.08715 to 0.08656, saving model to tmp_checkpoint.h5\n",
      "Epoch 63/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0706 - val_loss: 0.1445\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.08656\n",
      "Epoch 64/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0734 - val_loss: 0.0874\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.08656\n",
      "Epoch 65/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0710 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.08656 to 0.08587, saving model to tmp_checkpoint.h5\n",
      "Epoch 66/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0694 - val_loss: 0.0896\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.08587\n",
      "Epoch 67/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0653 - val_loss: 0.0937\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.08587\n",
      "Epoch 68/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0647 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.08587\n",
      "Epoch 69/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0655 - val_loss: 0.1012\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.08587\n",
      "Epoch 70/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0657 - val_loss: 0.0967\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.08587\n",
      "Epoch 71/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0725 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.08587\n",
      "Epoch 72/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0728 - val_loss: 0.1120\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.08587\n",
      "Epoch 73/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0680 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.08587 to 0.08537, saving model to tmp_checkpoint.h5\n",
      "Epoch 74/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0689 - val_loss: 0.0972\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.08537\n",
      "Epoch 75/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0609 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.08537 to 0.08353, saving model to tmp_checkpoint.h5\n",
      "Epoch 76/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0655 - val_loss: 0.0997\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.08353\n",
      "Epoch 77/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0965 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.08353 to 0.08030, saving model to tmp_checkpoint.h5\n",
      "Epoch 78/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0661 - val_loss: 0.1155\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.08030\n",
      "Epoch 79/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0647 - val_loss: 0.1184\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.08030\n",
      "Epoch 80/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0800 - val_loss: 0.0902\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.08030\n",
      "Epoch 81/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0665 - val_loss: 0.0922\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.08030\n",
      "Epoch 82/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0777 - val_loss: 0.1033\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.08030\n",
      "Epoch 83/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0835 - val_loss: 0.0848\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.08030\n",
      "Epoch 84/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0639 - val_loss: 0.1066\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.08030\n",
      "Epoch 85/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0627 - val_loss: 0.1260\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.08030\n",
      "Epoch 86/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0665 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.08030\n",
      "Epoch 87/5000\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0709 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.08030\n",
      "Epoch 88/5000\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0588 - val_loss: 0.0967\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.08030\n",
      "Epoch 89/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0598 - val_loss: 0.1127\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.08030\n",
      "Epoch 90/5000\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0595 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.08030\n",
      "Epoch 91/5000\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0573 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.08030\n",
      "Epoch 92/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0608 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.08030\n",
      "Epoch 93/5000\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0627 - val_loss: 0.1031\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.08030\n",
      "Epoch 94/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0574 - val_loss: 0.1477\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.08030\n",
      "Epoch 95/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0781 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.08030\n",
      "Epoch 96/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0548 - val_loss: 0.1299\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.08030\n",
      "Epoch 97/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0767 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.08030 to 0.07859, saving model to tmp_checkpoint.h5\n",
      "Epoch 98/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0636 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.07859\n",
      "Epoch 99/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0607 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.07859\n",
      "Epoch 100/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0625 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.07859\n",
      "Epoch 101/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0653 - val_loss: 0.1274\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.07859\n",
      "Epoch 102/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 24ms/step - loss: 0.1041 - val_loss: 0.1198\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.07859\n",
      "Epoch 103/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0559 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.07859 to 0.07537, saving model to tmp_checkpoint.h5\n",
      "Epoch 104/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0504 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.07537\n",
      "Epoch 105/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0512 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.07537\n",
      "Epoch 106/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0527 - val_loss: 0.0787\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.07537\n",
      "Epoch 107/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0494 - val_loss: 0.1481\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.07537\n",
      "Epoch 108/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0648 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.07537\n",
      "Epoch 109/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0658 - val_loss: 0.0793\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.07537\n",
      "Epoch 110/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0617 - val_loss: 0.0994\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.07537\n",
      "Epoch 111/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0534 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.07537\n",
      "Epoch 112/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0499 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.07537 to 0.07464, saving model to tmp_checkpoint.h5\n",
      "Epoch 113/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0469 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.07464\n",
      "Epoch 114/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0495 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.07464\n",
      "Epoch 115/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0479 - val_loss: 0.0736\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.07464 to 0.07358, saving model to tmp_checkpoint.h5\n",
      "Epoch 116/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0504 - val_loss: 0.0937\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.07358\n",
      "Epoch 117/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0581 - val_loss: 0.0927\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.07358\n",
      "Epoch 118/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0512 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.07358\n",
      "Epoch 119/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0572 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.07358\n",
      "Epoch 120/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0508 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.07358\n",
      "Epoch 121/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0509 - val_loss: 0.0767\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.07358\n",
      "Epoch 122/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0452 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.07358\n",
      "Epoch 123/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0496 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.07358\n",
      "Epoch 124/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0486 - val_loss: 0.1038\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.07358\n",
      "Epoch 125/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0542 - val_loss: 0.1294\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.07358\n",
      "Epoch 126/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0903 - val_loss: 0.0724\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.07358 to 0.07240, saving model to tmp_checkpoint.h5\n",
      "Epoch 127/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0640 - val_loss: 0.0757\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.07240\n",
      "Epoch 128/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0473 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.07240\n",
      "Epoch 129/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0614 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.07240\n",
      "Epoch 130/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0632 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.07240\n",
      "Epoch 131/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0467 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.07240\n",
      "Epoch 132/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0667 - val_loss: 0.1212\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.07240\n",
      "Epoch 133/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0668 - val_loss: 0.0927\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.07240\n",
      "Epoch 134/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0469 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.07240\n",
      "Epoch 135/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0486 - val_loss: 0.0757\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.07240\n",
      "Epoch 136/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0432 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.07240\n",
      "Epoch 137/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0511 - val_loss: 0.1083\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.07240\n",
      "Epoch 138/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0481 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.07240\n",
      "Epoch 139/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0450 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.07240\n",
      "Epoch 140/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0568 - val_loss: 0.0943\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.07240\n",
      "Epoch 141/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0488 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.07240\n",
      "Epoch 142/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0481 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.07240\n",
      "Epoch 143/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0511 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.07240\n",
      "Epoch 144/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0561 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.07240\n",
      "Epoch 145/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0504 - val_loss: 0.1363\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.07240\n",
      "Epoch 146/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0734 - val_loss: 0.0722\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.07240 to 0.07224, saving model to tmp_checkpoint.h5\n",
      "Epoch 147/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0801 - val_loss: 0.0699\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.07224 to 0.06988, saving model to tmp_checkpoint.h5\n",
      "Epoch 148/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0480 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.06988\n",
      "Epoch 149/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0478 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.06988\n",
      "Epoch 150/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0457 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.06988\n",
      "Epoch 151/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0436 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.06988\n",
      "Epoch 152/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0483 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.06988\n",
      "Epoch 153/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0700 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.06988\n",
      "Epoch 154/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0431 - val_loss: 0.0740\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.06988\n",
      "Epoch 155/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0463 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.06988\n",
      "Epoch 156/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0428 - val_loss: 0.1102\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.06988\n",
      "Epoch 157/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0598 - val_loss: 0.0793\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.06988\n",
      "Epoch 158/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0568 - val_loss: 0.0957\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.06988\n",
      "Epoch 159/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0732 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.06988\n",
      "Epoch 160/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0432 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.06988\n",
      "Epoch 161/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0522 - val_loss: 0.0706\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.06988\n",
      "Epoch 162/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0446 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.06988\n",
      "Epoch 163/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0447 - val_loss: 0.0765\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.06988\n",
      "Epoch 164/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0426 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.06988\n",
      "Epoch 165/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0420 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.06988\n",
      "Epoch 166/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0428 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.06988\n",
      "Epoch 167/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0466 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.06988\n",
      "Epoch 168/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0415 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.06988\n",
      "Epoch 169/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0384 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.06988\n",
      "Epoch 170/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0402 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.06988\n",
      "Epoch 171/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0497 - val_loss: 0.1005\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.06988\n",
      "Epoch 172/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0481 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.06988\n",
      "Epoch 173/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0394 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.06988\n",
      "Epoch 174/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0382 - val_loss: 0.0753\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.06988\n",
      "Epoch 175/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0407 - val_loss: 0.0747\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.06988\n",
      "Epoch 176/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0452 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.06988\n",
      "Epoch 177/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0400 - val_loss: 0.1025\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.06988\n",
      "Epoch 178/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0595 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.06988\n",
      "Epoch 179/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0459 - val_loss: 0.0920\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.06988\n",
      "Epoch 180/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0426 - val_loss: 0.0732\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.06988\n",
      "Epoch 181/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0381 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.06988\n",
      "Epoch 182/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0520 - val_loss: 0.0740\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.06988\n",
      "Epoch 183/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0462 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.06988\n",
      "Epoch 184/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0425 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.06988\n",
      "Epoch 185/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0371 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.06988\n",
      "Epoch 186/5000\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0428 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.06988\n",
      "Epoch 187/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0413 - val_loss: 0.0705\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.06988\n",
      "Epoch 188/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0408 - val_loss: 0.0705\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.06988\n",
      "Epoch 189/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0585 - val_loss: 0.1630\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.06988\n",
      "Epoch 190/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0765 - val_loss: 0.0962\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.06988\n",
      "Epoch 191/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0485 - val_loss: 0.0726\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.06988\n",
      "Epoch 192/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0498 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.06988\n",
      "Epoch 193/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0460 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.06988\n",
      "Epoch 194/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0515 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.06988\n",
      "Epoch 195/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0451 - val_loss: 0.0742\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.06988\n",
      "Epoch 196/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0416 - val_loss: 0.0725\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.06988\n",
      "Epoch 197/5000\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0355 - val_loss: 0.0906\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.06988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 23:18:09.562296: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "num=0\n",
    "fit_num=1000\n",
    "for i in range(fit_num):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(layer_num, \n",
    "                   input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "                   activation='relu', \n",
    "                   return_sequences=False)\n",
    "              )\n",
    "    sgd=optimizers.SGD(lr=0.001)\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    filename =  'tmp_checkpoint.h5'\n",
    "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        epochs=5000, \n",
    "                        batch_size=30,\n",
    "                        validation_data=(x_valid, y_valid), \n",
    "                        callbacks=[early_stop, checkpoint])\n",
    "    \n",
    "    aa=pd.DataFrame(model.predict(train_feature),columns={'a'})\n",
    "    bb=pd.DataFrame(model.predict(test_feature),columns={'a'})\n",
    "    result_GRU['Simulation_'+ str(i)]=aa['a']\n",
    "    predict_GRU['Simulation_'+ str(i)]=bb['a']\n",
    "    num=num+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:51.658146Z",
     "start_time": "2021-05-17T05:27:51.644192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Week</th>\n",
       "      <th>warning</th>\n",
       "      <th>Simulation_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.034391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.966943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.121620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>2016</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.195673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>2016</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1.113575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>2016</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1.120593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>2016</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1.196418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2016</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1.186449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Week  warning  Simulation_0\n",
       "0    2009     3        1      0.991255\n",
       "1    2009     4        1      1.034391\n",
       "2    2009     5        1      1.000608\n",
       "3    2009     6        1      0.966943\n",
       "4    2009     7        1      1.121620\n",
       "..    ...   ...      ...           ...\n",
       "417  2016    49        1      1.195673\n",
       "418  2016    50        1      1.113575\n",
       "419  2016    51        1      1.120593\n",
       "420  2016    52        1      1.196418\n",
       "421  2016    53        1      1.186449\n",
       "\n",
       "[422 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:52.391168Z",
     "start_time": "2021-05-17T05:27:52.377081Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Week</th>\n",
       "      <th>warning</th>\n",
       "      <th>Simulation_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.098290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.208818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.246450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.079130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2018</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.038697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2018</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2018</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2018</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1.190986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2018</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1.217417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Week  warning  Simulation_0\n",
       "0    2017     1        1      1.098290\n",
       "1    2017     2        1      1.208818\n",
       "2    2017     3        1      1.246450\n",
       "3    2017     4        1      1.079130\n",
       "4    2017     5        1      1.091727\n",
       "..    ...   ...      ...           ...\n",
       "101  2018    49        1      1.038697\n",
       "102  2018    50        1      0.885603\n",
       "103  2018    51        1      0.896249\n",
       "104  2018    52        1      1.190986\n",
       "105  2018    53        1      1.217417\n",
       "\n",
       "[106 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:53.122539Z",
     "start_time": "2021-05-17T05:27:53.108976Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the results\n",
    "result_sum=result_GRU.iloc[:,result_GRU.columns.str.contains(\"Sim\")]\n",
    "predict_sum=predict_GRU.iloc[:,predict_GRU.columns.str.contains(\"Sim\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sum.to_excel('result_sum_2007_gru.xlsx',index=None)\n",
    "predict_sum.to_excel('predict_sum_2007_gru.xlsx',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:53.867669Z",
     "start_time": "2021-05-17T05:27:53.852884Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/ffbj5k35735bbppg5hsklzs40000gn/T/ipykernel_45123/3059916937.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_sum['warning']=train_label\n",
      "/var/folders/0t/ffbj5k35735bbppg5hsklzs40000gn/T/ipykernel_45123/3059916937.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_sum['warning']=test_label\n"
     ]
    }
   ],
   "source": [
    "result_sum['warning']=train_label\n",
    "predict_sum['warning']=test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:55.360740Z",
     "start_time": "2021-05-17T05:27:55.346738Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/ffbj5k35735bbppg5hsklzs40000gn/T/ipykernel_45123/531947779.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_sum['GRU_mean']=r_tmp.mean(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# train result\n",
    "r_tmp=result_GRU.iloc[:,result_GRU.columns.str.contains(\"Sim\")]\n",
    "result_sum['GRU_mean']=r_tmp.mean(axis=1)\n",
    "result_sum['GRU_95+']=r_tmp.quantile(axis=1,q=0.975)\n",
    "result_sum['GRU_95-']=r_tmp.quantile(axis=1,q=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:56.141625Z",
     "start_time": "2021-05-17T05:27:56.127022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/ffbj5k35735bbppg5hsklzs40000gn/T/ipykernel_45123/159267740.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_sum['GRU_mean']=p_tmp.mean(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# test result\n",
    "p_tmp=predict_GRU.iloc[:,predict_GRU.columns.str.contains(\"Sim\")]\n",
    "predict_sum['GRU_mean']=p_tmp.mean(axis=1)\n",
    "predict_sum['GRU_95+']=p_tmp.quantile(axis=1,q=0.975)\n",
    "predict_sum['GRU_95-']=p_tmp.quantile(axis=1,q=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:27:59.119793Z",
     "start_time": "2021-05-17T05:27:59.104934Z"
    }
   },
   "outputs": [],
   "source": [
    "p_v=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_sum.index:\n",
    "    if (result_sum.loc[i,'GRU_95+']>=p_v):\n",
    "        result_sum.loc[i,'prediction_95+']=1\n",
    "    else:\n",
    "        result_sum.loc[i,'prediction_95+']=0\n",
    "\n",
    "for i in predict_sum.index:\n",
    "    if (predict_sum.loc[i,'GRU_95+']>=p_v):\n",
    "        predict_sum.loc[i,'prediction_95+']=1\n",
    "    else:\n",
    "        predict_sum.loc[i,'prediction_95+']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_sum.index:\n",
    "    if (result_sum.loc[i,'GRU_95-']>=p_v):\n",
    "        result_sum.loc[i,'prediction_95-']=1\n",
    "    else:\n",
    "        result_sum.loc[i,'prediction_95-']=0\n",
    "\n",
    "for i in predict_sum.index:\n",
    "    if (predict_sum.loc[i,'GRU_95-']>=p_v):\n",
    "        predict_sum.loc[i,'prediction_95-']=1\n",
    "    else:\n",
    "        predict_sum.loc[i,'prediction_95-']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T05:28:00.208309Z",
     "start_time": "2021-05-17T05:27:59.898512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995261</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.922705</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy    Recall Precision     F1\n",
       "0  0.995261  0.989637  0.922705  0.955"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.90566</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  accuracy    Recall Precision        F1\n",
       "0  0.90566  0.852459  0.881356  0.866667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in result_sum.index:\n",
    "    if (result_sum.loc[i,'GRU_mean']>=p_v):\n",
    "        result_sum.loc[i,'prediction']=1\n",
    "    else:\n",
    "        result_sum.loc[i,'prediction']=0\n",
    "\n",
    "for i in predict_sum.index:\n",
    "    if (predict_sum.loc[i,'GRU_mean']>=p_v):\n",
    "        predict_sum.loc[i,'prediction']=1\n",
    "    else:\n",
    "        predict_sum.loc[i,'prediction']=0\n",
    "\n",
    "Result_ACC_train=pd.DataFrame(columns=['accuracy','Recall','Precision','F1'])\n",
    "Result_ACC_test=pd.DataFrame(columns=['accuracy','Recall','Precision','F1'])\n",
    "\n",
    "result_sum.loc[(result_sum['warning']==result_sum['prediction']),'equality']=1\n",
    "predict_sum.loc[(predict_sum['warning']==predict_sum['prediction']),'equality']=1\n",
    "\n",
    "result_sum=result_sum.fillna(0)\n",
    "predict_sum=predict_sum.fillna(0)\n",
    "\n",
    "Result_ACC_train.loc[0,'accuracy']=result_sum['equality'].mean()\n",
    "Result_ACC_test.loc[0,'accuracy']=predict_sum['equality'].mean()\n",
    "\n",
    "Result_ACC_train.loc[0,'Recall']=len(result_sum[(result_sum['warning']==1) & (result_sum['prediction']==1)])/len(result_sum[(result_sum['warning']==1)])\n",
    "Result_ACC_train.loc[0,'Precision']=len(result_sum[(result_sum['warning']==1) & (result_sum['prediction']==1)])/len(result_sum[(result_sum['prediction']==1)])\n",
    "Result_ACC_train.loc[0,'F1']=2*Result_ACC_train.loc[0,'Recall']*Result_ACC_train.loc[0,'Precision']/(Result_ACC_train.loc[0,'Recall']+Result_ACC_train.loc[0,'Precision'])\n",
    "\n",
    "Result_ACC_test.loc[0,'Recall']=len(predict_sum[(predict_sum['warning']==1) & (predict_sum['prediction']==1)])/len(predict_sum[(predict_sum['warning']==1)])\n",
    "Result_ACC_test.loc[0,'Precision']=len(predict_sum[(predict_sum['warning']==1) & (predict_sum['prediction']==1)])/len(predict_sum[(predict_sum['prediction']==1)])\n",
    "Result_ACC_test.loc[0,'F1']=2*Result_ACC_test.loc[0,'Recall']*Result_ACC_test.loc[0,'Precision']/(Result_ACC_test.loc[0,'Recall']+Result_ACC_test.loc[0,'Precision'])\n",
    "\n",
    "display(Result_ACC_train)\n",
    "\n",
    "display(Result_ACC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_save=result_sum[['warning', 'prediction_95-','prediction','prediction_95+']]\n",
    "predict_save=predict_sum[['warning', 'prediction_95-','prediction','prediction_95+']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_save.to_excel('result_save_gru.xlsx',index=None)\n",
    "predict_save.to_excel('predict_save_gru.xlsx',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "275.667px",
    "left": "910px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
